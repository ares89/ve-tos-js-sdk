import axios from 'axios';
import TOS, { isCancel } from '../../src/browser-index';
import { deleteBucket, sleepCache, NEVER_TIMEOUT } from '../utils';
import {
  testBucketName,
  isNeedDeleteBucket,
  tosOptions,
} from '../utils/options';
import fs from 'fs';
import fsPromises from '../../src/nodejs/fs-promises';
import path from 'path';
import {
  CheckpointRecord,
  UploadEvent,
  UploadEventType,
  UploadFileInput,
} from '../../src/methods/object/multipart/uploadFile';

const autoGeneratedObjectsDir = path.resolve(
  __dirname,
  './auto-generated-objects'
);
fs.mkdirSync(autoGeneratedObjectsDir, { recursive: true });
const objectKey100M = 'test-object-100M.txt';
const objectPath100M = path.resolve(autoGeneratedObjectsDir, objectKey100M);
const objectKey1K = 'test-object-1K.txt';
const objectPath1K = path.resolve(autoGeneratedObjectsDir, objectKey1K);
if (!fs.existsSync(objectPath100M)) {
  fs.writeFileSync(
    objectPath100M,
    Buffer.alloc(100 * 1024 * 1024, 'a'),
    'binary'
  );
}
if (!fs.existsSync(objectPath1K)) {
  fs.writeFileSync(objectPath1K, Buffer.alloc(1 * 1024, 'a'), 'binary');
}

const checkpointsDir =
  path.resolve(__dirname, 'auto-generated-checkpoints') + '/';

describe('uploadFile in node.js environment', () => {
  beforeAll(async done => {
    const client = new TOS(tosOptions);
    // clear all bucket
    const { data: buckets } = await client.listBuckets();
    for (const bucket of buckets.Buckets) {
      if (isNeedDeleteBucket(bucket.Name)) {
        try {
          await deleteBucket(client, bucket.Name);
        } catch (err) {
          console.log('a: ', err);
        }
      }
    }
    // create bucket
    await client.createBucket({
      bucket: testBucketName,
    });
    await sleepCache();
    done();
  }, NEVER_TIMEOUT);
  // afterAll(async done => {
  //   const client = new TOS(tosOptions);
  //   console.log('delete bucket.....');
  //   // delete bucket
  //   deleteBucket(client, testBucketName);
  //   done();
  // }, NEVER_TIMEOUT);

  it(
    'small file without checkpoint',
    async () => {
      const key = `${objectKey1K}_without_checkpoint`;
      const client = new TOS(tosOptions);
      await client.uploadFile({ file: objectPath1K, key });
      const { data } = await client.getObject(key);
      expect(data.length === 1024);
    },
    NEVER_TIMEOUT
  );

  let needTimeOneTask = 0;
  let needTimeFourTask = 0;

  it(
    'without checkpoint',
    async () => {
      const key = `${objectKey100M}-without-checkpoint`;
      const client = new TOS(tosOptions);
      const startDate = new Date();
      await client.uploadFile({ file: objectPath100M, key });
      const endDate = new Date();
      needTimeOneTask = endDate.valueOf() - startDate.valueOf();
      const { data } = await client.headObject(key);
      expect(+data['content-length'] === 100 * 1024 * 1024);
    },
    NEVER_TIMEOUT
  );

  it(
    'taskNum is four and partSize is 25MB',
    async () => {
      const key = `${objectKey100M}_parallel_4`;
      const client = new TOS(tosOptions);
      const startDate = new Date();
      await client.uploadFile({
        file: objectPath100M,
        key,
        partSize: 25 * 1024 * 1024,
        taskNum: 4,
      });
      const endDate = new Date();
      needTimeFourTask = endDate.valueOf() - startDate.valueOf();
      expect(2 * needTimeFourTask < needTimeOneTask);
      const { data } = await client.headObject(key);
      expect(+data['content-length'] === 100 * 1024 * 1024);
    },
    NEVER_TIMEOUT
  );

  it(
    'small file with checkpoint file',
    async () => {
      const key = `${objectKey1K}_with_checkpoint_file`;
      const client = new TOS(tosOptions);
      const uploadEventChangeFn = jest.fn();
      const progressFn = jest.fn();
      await client.uploadFile({
        file: objectPath1K,
        key,
        checkpoint: checkpointsDir,
        uploadEventChange: uploadEventChangeFn,
        progress: progressFn,
      });
      expect(uploadEventChangeFn.mock.calls.length).toBe(3);
      expect(uploadEventChangeFn.mock.calls[0][0].type).toBe(
        UploadEventType.createMultipartUploadSucceed
      );
      const checkpointFilePath =
        uploadEventChangeFn.mock.calls[0][0].checkpointFile;
      expect(checkpointFilePath).not.toBeUndefined();
      expect(uploadEventChangeFn.mock.calls[0][0].type).toBe(
        UploadEventType.createMultipartUploadSucceed
      );

      expect(progressFn.mock.calls.length).toBe(2);
      expect(progressFn.mock.calls[0][0]).toBe(0);
      expect(progressFn.mock.calls[1][0]).toBe(1);

      const { data } = await client.getObject(key);
      expect(data.length === 1024);
    },
    NEVER_TIMEOUT
  );

  it(
    'with specific checkpoint filename',
    async () => {
      const key = `${objectKey1K}_with_specific_checkpoint_file`;
      const client = new TOS(tosOptions);
      const uploadEventChangeFn = jest.fn();
      const filepath = path.resolve(
        checkpointsDir,
        'specific_checkpoint_file.json'
      );

      await client.uploadFile({
        file: objectPath1K,
        key,
        checkpoint: filepath,
        uploadEventChange: uploadEventChangeFn,
      });

      expect(uploadEventChangeFn.mock.calls[0][0].checkpointFile).toBe(
        filepath
      );
    },
    NEVER_TIMEOUT
  );

  it(
    'pause and resume with checkpoint',
    async () => {
      const key = `${objectKey100M}-pause-and-resume-with-checkpoint`;
      const client = new TOS(tosOptions);
      const cpFilepath = path.resolve(
        checkpointsDir,
        'pause-and-resume-checkpoint.json'
      );
      await fsPromises.rm(cpFilepath, { force: true });

      let resolve = (_v?: unknown) => {};
      const p = new Promise(r => (resolve = r));
      const pausePartCount = 4;
      const allPartCount = 10;
      let currentPartCount = 0;
      const source = axios.CancelToken.source();
      const uploadEventChange = (e: UploadEvent) => {
        if (e.type === UploadEventType.uploadPartSucceed) {
          ++currentPartCount;

          if (currentPartCount === pausePartCount) {
            source.cancel('');
            setTimeout(resolve, 1000);
          }
        }
      };

      const uploadFilePromise = client.uploadFile({
        file: objectPath100M,
        key,
        checkpoint: cpFilepath,
        uploadEventChange,
        partSize: (100 * 1024 * 1024) / allPartCount,
        cancelToken: source.token,
      });
      await uploadFilePromise.catch(err => {
        if (!isCancel(err)) {
          console.log(err);
        }
        expect(isCancel(err)).toBeTruthy();
      });
      const checkpointFileContent: CheckpointRecord = require(cpFilepath);
      const uploadedPartCount = checkpointFileContent.parts_info?.length || 0;

      // first write file, then call callback
      // so there maybe be more part
      expect(uploadedPartCount).toBeGreaterThanOrEqual(pausePartCount);

      await p;
      const uploadEventChangeFn = jest.fn();
      await client.uploadFile({
        file: objectPath100M,
        key,
        checkpoint: cpFilepath,
        uploadEventChange: uploadEventChangeFn,
      });

      expect(
        uploadEventChangeFn.mock.calls.filter(
          it => it[0].type === UploadEventType.uploadPartSucceed
        ).length
      ).toBe(allPartCount - uploadedPartCount);
      expect(
        uploadEventChangeFn.mock.calls.filter(
          it => it[0].type === UploadEventType.completeMultipartUploadSucceed
        ).length
      ).toBe(1);

      const { data } = await client.headObject(key);
      expect(+data['content-length'] === 100 * 1024 * 1024);
    },
    NEVER_TIMEOUT
  );

  it(
    'pause and resume with checkpoint when partNum is 3',
    async () => {
      const key = `${objectKey100M}-pause-and-resume-with-checkpoint-when-partNum-is-3`;
      const client = new TOS(tosOptions);
      const cpFilepath = path.resolve(
        checkpointsDir,
        'pause-and-resume-checkpoint-when-partNum-is-3.json'
      );
      await fsPromises.rm(cpFilepath, { force: true });

      let resolve = (_v?: unknown) => {};
      const p = new Promise(r => (resolve = r));
      const pausePartCount = 4;
      const allPartCount = 10;
      let currentPartCount = 0;
      const source = axios.CancelToken.source();
      const uploadEventChange = (e: UploadEvent) => {
        if (e.type === UploadEventType.uploadPartSucceed) {
          ++currentPartCount;

          if (currentPartCount === pausePartCount) {
            source.cancel('');
            setTimeout(resolve, 1000);
          }
        }
      };

      const uploadFilePromise = client.uploadFile({
        file: objectPath100M,
        key,
        checkpoint: cpFilepath,
        uploadEventChange,
        partSize: (100 * 1024 * 1024) / allPartCount,
        cancelToken: source.token,
        taskNum: 3,
      });
      await uploadFilePromise.catch(err => {
        if (!isCancel(err)) {
          console.log(err);
        }
        expect(isCancel(err)).toBeTruthy();
      });
      const checkpointFileContent: CheckpointRecord = require(cpFilepath);
      const uploadedPartCount = checkpointFileContent.parts_info?.length || 0;

      // first write file, then call callback
      // so there maybe be more part
      expect(uploadedPartCount).toBeGreaterThanOrEqual(pausePartCount);

      await p;
      const uploadEventChangeFn = jest.fn();
      await client.uploadFile({
        file: objectPath100M,
        key,
        checkpoint: cpFilepath,
        uploadEventChange: uploadEventChangeFn,
        taskNum: 3,
      });

      expect(
        uploadEventChangeFn.mock.calls.filter(
          it => it[0].type === UploadEventType.uploadPartSucceed
        ).length
      ).toBe(allPartCount - uploadedPartCount);
      expect(
        uploadEventChangeFn.mock.calls.filter(
          it => it[0].type === UploadEventType.completeMultipartUploadSucceed
        ).length
      ).toBe(1);

      const { data } = await client.headObject(key);
      expect(+data['content-length'] === 100 * 1024 * 1024);
    },
    NEVER_TIMEOUT
  );

  it(
    'pause and resume without checkpoint file',
    async () => {
      const key = `${objectKey100M}-pause-and-resume-without-checkpoint-file`;
      const client = new TOS(tosOptions);

      let resolve = (_v?: unknown) => {};
      const p = new Promise(r => (resolve = r));
      const pausePartCount = 5;
      const allPartCount = 10;
      const source = axios.CancelToken.source();
      let abortedCheckpoint: CheckpointRecord = (null as unknown) as CheckpointRecord;
      const progress: UploadFileInput['progress'] = (p, cp) => {
        if (p >= 0.5) {
          abortedCheckpoint = cp;
          source.cancel();

          setTimeout(resolve, 1000);
        }
      };

      const uploadFilePromise = client.uploadFile({
        file: objectPath100M,
        key,
        progress,
        partSize: (100 * 1024 * 1024) / allPartCount,
        cancelToken: source.token,
      });
      await uploadFilePromise.catch(err => {
        if (!isCancel(err)) {
          console.log(err);
        }
        expect(isCancel(err)).toBeTruthy();
      });

      // first write file, then call callback
      // so there maybe be more part
      expect(abortedCheckpoint.parts_info?.length).toBeGreaterThanOrEqual(
        pausePartCount
      );

      await p;
      await client.uploadFile({
        file: objectPath100M,
        key,
        checkpoint: abortedCheckpoint,
      });

      const { data } = await client.headObject(key);
      expect(+data['content-length'] === 100 * 1024 * 1024);
    },
    NEVER_TIMEOUT
  );

  it('modify file after pause', async () => {}, NEVER_TIMEOUT);

  it(
    "partSize param is not equal to checkpoint file's partSize",
    async () => {},
    NEVER_TIMEOUT
  );

  // first upload part will fail
  it('uploadId of checkpoint file is aborted', async () => {}, NEVER_TIMEOUT);
});
